#!/usr/bin/env python3
"""
Regression tests for continuous-time entry/exit model.

This test suite ensures numerical consistency of model results
across code changes.  It uses pre-generated baseline values to
verify that model components produce the expected results.
The baseline values are generated by `generate_test_baselines.py`,
which uses different parameters or configurations to produce
known good values and stores them for regression testing.
These are stored in a JSON file named `test_baselines.json`.
"""

import pytest
import numpy as np
from conftest import create_model

from model import Model


def test_algorithm_consistency(baselines, tolerances):
    """Test that different algorithms produce consistent results."""
    algorithms = list(baselines['algorithm_baselines'].keys())
    assert len(algorithms) == 4, "At least four variations should be tested"
    reference_algo = 'value_iteration'
    assert reference_algo in algorithms, f"Reference algorithm {reference_algo} not found in baselines"

    ref_baseline = baselines['algorithm_baselines'][reference_algo]
    ref_results = ref_baseline['results']
    ref_v = np.array(ref_results['value_function']['values'])
    ref_p = np.array(ref_results['choice_probabilities']['values'])
    ref_ll = ref_results['log_likelihood']['value']
    ref_grad = np.array(ref_results['log_likelihood']['gradient'])

    for algo_name in algorithms:
        baseline = baselines['algorithm_baselines'][algo_name]
        algo_config = baseline['algorithm_config']
        test_config = baseline['test_config']
        expected = baseline['results']['value_function']

        # Create model with specific algorithm configuration
        model = Model(
            n_players=test_config['n_players'],
            n_demand=test_config['n_demand'],
            param=test_config['params'],
            rho=test_config['rho'],
            verbose=False,
            config=algo_config,
        )

        # Compute value function
        v, dv = model.value_function()

        # Test sample values
        assert np.isclose(
            v.flat[0], expected['sample_values']['v[0]'],
            rtol=tolerances['value_function']
        ), f"v[0] mismatch for algorithm {algo_name}"
        assert np.isclose(
            np.mean(v), expected['sample_values']['mean'],
            rtol=tolerances['value_function']
        ), f"Mean value function mismatch for algorithm {algo_name}"

        # Test derivatives
        param_keys = ['theta_ec', 'theta_rn', 'theta_d', 'lambda', 'gamma']
        for param_name in param_keys:
            np.testing.assert_allclose(
                dv[param_name], expected['derivatives'][param_name],
                rtol=tolerances['value_function'],
                err_msg=f"Value function derivative mismatch for {param_name} in algorithm {algo_name}"
            )

        # Test value function values
        np.testing.assert_allclose(
            v, expected['values'],
            rtol=tolerances['value_function'],
            err_msg=f"Value function mismatch for algorithm {algo_name}"
        )

        if algo_name != reference_algo:
            test_results = baseline['results']
            test_v = np.array(test_results['value_function']['values'])
            test_p = np.array(test_results['choice_probabilities']['values'])
            test_ll = test_results['log_likelihood']['value']
            test_grad = np.array(test_results['log_likelihood']['gradient'])

            # Test consistency with reference algorithm
            np.testing.assert_allclose(
                test_v, ref_v,
                atol=tolerances['algorithm_consistency'],
                err_msg=f"Value function inconsistency between {algo_name} and {reference_algo}"
            )
            np.testing.assert_allclose(
                test_p, ref_p,
                atol=tolerances['algorithm_consistency'],
                err_msg=f"Choice probability inconsistency between {algo_name} and {reference_algo}"
            )
            assert np.isclose(
                test_ll, ref_ll,
                atol=tolerances['algorithm_consistency']
            ), f"Log-likelihood inconsistency between {algo_name} and {reference_algo}"
            np.testing.assert_allclose(
                test_grad, ref_grad,
                atol=tolerances['algorithm_consistency'],
                err_msg=f"Log-likelihood gradient inconsistency between {algo_name} and {reference_algo}"
            )


@pytest.mark.parametrize("param_set_name", [
    'small',
    'baseline',
    'large',
    'high_competition',
    'low_entry_cost',
    'strong_competition',
    'near_divergent',
])
def test_baselines(param_set_name, baselines, tolerances):
    """Test baseline values of all structural components and derivatives."""
    baseline = baselines['baselines'][param_set_name]
    config = baseline['config']

    # Create model instance
    model = create_model(config, param_set_name)

    # Test value function and derivatives
    # -----------------------------------

    v, dv = model.value_function()
    expected = baseline['results']['value_function']

    # Test sample value function values
    assert np.isclose(
        v.flat[0], expected['sample_values']['v[0]'],
        atol=tolerances['value_function']
    ), f"v[0] mismatch for {param_set_name}"
    assert np.isclose(
        v.flat[-1], expected['sample_values']['v[-1]'],
        atol=tolerances['value_function']
    ), f"v[-1] mismatch for {param_set_name}"
    assert np.isclose(
        np.mean(v), expected['sample_values']['mean'],
        atol=tolerances['value_function']
    ), f"Mean value function mismatch for {param_set_name}"

    # Test all value function values
    np.testing.assert_allclose(
        v, expected['values'],
        atol=tolerances['value_function'],
        err_msg=f"Value function mismatch for {param_set_name}"
    )

    # Test value function derivatives ∂v/∂θ
    for param_name in model.param_keys:
        np.testing.assert_allclose(
            dv[param_name], expected['derivatives'][param_name],
            atol=tolerances['derivative'],
            err_msg=f"Value function derivative mismatch for {param_name} in {param_set_name}"
        )

    # Test choice probabilities and derivatives
    # -----------------------------------------

    p, dp = model.choice_probabilities(v, dv)
    expected = baseline['results']['choice_probabilities']

    # Test sample statistics
    assert np.isclose(
        np.mean(p), expected['sample_values']['mean'],
        atol=tolerances['choice_prob']
    ), f"Mean choice probability mismatch for {param_set_name}"
    assert np.isclose(
        np.min(p), expected['sample_values']['min'],
        atol=tolerances['choice_prob']
    ), f"Min choice probability mismatch for {param_set_name}"
    assert np.isclose(
        np.max(p), expected['sample_values']['max'],
        atol=tolerances['choice_prob']
    ), f"Max choice probability mismatch for {param_set_name}"

    # Test all choice probabilities
    np.testing.assert_allclose(
        p, expected['values'],
        atol=tolerances['choice_prob'],
        err_msg=f"Choice probabilities mismatch for {param_set_name}"
    )

    # Test choice probability derivatives
    for param_name in model.param_keys:
        np.testing.assert_allclose(
            dp[param_name], expected['derivatives'][param_name],
            atol=tolerances['choice_prob'],
            err_msg=f"Choice probability derivative mismatch for {param_name} in {param_set_name}"
        )

    # Test intensity matrix and derivatives
    # -------------------------------------

    Q, dQ = model.intensity_matrix()
    expected = baseline['results']['intensity_matrix']

    # Test intensity matrix properties
    assert Q.shape == tuple(expected['shape']), \
        f"Intensity matrix shape mismatch for {param_set_name}"
    assert Q.nnz == expected['nnz'], \
        f"Intensity matrix nnz mismatch for {param_set_name}"

    # Test intensity matrix diagonal
    np.testing.assert_allclose(
        Q.diagonal(), expected['diagonal'],
        atol=tolerances['intensity'],
        err_msg=f"Intensity matrix diagonal mismatch for {param_set_name}"
    )

    # Test row sums (should be close to 0)
    row_sums = Q.toarray().sum(axis=1)
    np.testing.assert_allclose(
        row_sums, expected['row_sums'],
        atol=tolerances['intensity'],
        err_msg=f"Intensity matrix row sums mismatch for {param_set_name}"
    )

    # Test specific elements
    Q_dense = Q.toarray()
    for key, expected_val in expected['sample_values'].items():
        # Parse indices from key like 'Q[0,1]'
        indices = key[2:-1].split(',')
        i, j = int(indices[0]), int(indices[1])
        assert np.isclose(
            Q_dense[i, j], expected_val,
            atol=tolerances['intensity']
        ), f"{key} mismatch for {param_set_name}"

    # Test intensity matrix derivative sparsity
    for param_name in model.param_keys:
        assert dQ[param_name].nnz == expected['derivatives_nnz'][param_name], \
            f"Intensity matrix derivative nnz mismatch for {param_name} in {param_set_name}"

    # Test intensity matrix derivative sample values
    for key, expected_val in expected['sample_derivatives'].items():
        # Parse indices from key like 'dQ/dgamma[14,15]'
        param_name = key.split('/')[1].split('[')[0][1:]  # extract 'gamma'
        indices = key.split('[')[1][:-1].split(',')       # extract ['14', '15']
        i, j = int(indices[0]), int(indices[1])           # extract 14, 15
        assert np.isclose(
            dQ[param_name].toarray()[i, j], expected_val,
            atol=tolerances['derivative']
        ), f"{key} mismatch for {param_set_name}"

    # Test DGP and Log Likelihood Function
    # ------------------------------------

    expected = baseline['results']['log_likelihood']
    dgp_seed = expected['dgp_seed']
    Delta = expected['Delta']
    test_data = model.discrete_time_dgp(n_obs=expected['n_obs'], Delta=1.0, seed=dgp_seed)
    param_array = np.array([config['params'][key] for key in model.param_keys])
    ll, grad = model.log_likelihood(param_array, test_data, Delta=Delta, grad=True)

    # Test sample values
    test_sample = [int(s) for s in test_data]  # convert np.integer to int
    assert test_sample == expected['sample']

    # Test log-likelihood value
    assert np.isclose(
        ll, expected['value'],
        atol=tolerances['log_likelihood']
    ), f"Log-likelihood mismatch for {param_set_name}"

    # Test log-likelihood gradient
    np.testing.assert_allclose(
        grad, expected['gradient'],
        atol=tolerances['gradient'],
        err_msg=f"Log-likelihood gradient mismatch for {param_set_name}"
    )

    # Test Bellman operator
    # ---------------------

    expected = baseline['results']['bellman_operator']

    # Apply Bellman operator once
    v_new = model.bellman_operator(v)

    # Test summary measures
    max_change = np.max(np.abs(v_new - v))
    mean_change = np.mean(np.abs(v_new - v))
    assert np.isclose(
        max_change, expected['max_change'],
        atol=tolerances['value_function']
    ), f"Bellman operator max change mismatch for {param_set_name}"
    assert np.isclose(
        mean_change, expected['mean_change'],
        atol=tolerances['value_function']
    ), f"Bellman operator mean change mismatch for {param_set_name}"

    # Test sample values
    assert np.isclose(
        v_new.flat[0], expected['sample_values']['v_new[0]'],
        atol=tolerances['value_function']
    ), f"v_new[0] mismatch for {param_set_name}"
    assert np.isclose(
        v_new.flat[-1], expected['sample_values']['v_new[-1]'],
        atol=tolerances['value_function']
    ), f"v_new[-1] mismatch for {param_set_name}"

    # Test all value function values
    np.testing.assert_allclose(
        v_new, expected['values'],
        atol=tolerances['value_function'],
        err_msg=f"Bellman operator mismatch for {param_set_name}"
    )

    # Test Bellman operator derivatives ∂T/∂v
    # ---------------------------------------

    # Compute Jacobian
    dT_dv = model.dbellman_operator_dv(v)
    expected = baseline['results']['dbellman_operator_dv']

    # Test matrix properties
    assert dT_dv.shape == tuple(expected['shape']), \
        f"Jacobian shape mismatch for {param_set_name}"
    assert dT_dv.nnz == expected['nnz'], \
        f"Jacobian nnz mismatch for {param_set_name}"
    assert dT_dv.format == expected['format'], \
        f"Jacobian format mismatch for {param_set_name}"

    # Test sample values
    assert np.isclose(
        dT_dv[0, 0], expected['sample_values']['dT_dv[0,0]'],
        atol=tolerances['derivative']
    ), f"dT_dv[0,0] mismatch for {param_set_name}"
    assert np.isclose(
        dT_dv[0, 1], expected['sample_values']['dT_dv[0,1]'],
        atol=tolerances['derivative']
    ), f"dT_dv[0,1] mismatch for {param_set_name}"
    assert np.isclose(
        dT_dv[1, 2], expected['sample_values']['dT_dv[1,2]'],
        atol=tolerances['derivative']
    ), f"dT_dv[1,2] mismatch for {param_set_name}"
    assert np.isclose(
        dT_dv[-1, -1], expected['sample_values']['dT_dv[-1,-1]'],
        atol=tolerances['derivative']
    ), f"dT_dv[-1,-1] mismatch for {param_set_name}"

    # Test diagonal statistics
    assert np.isclose(
        np.max(dT_dv.diagonal()), expected['sample_values']['max_diagonal'],
        atol=tolerances['derivative']
    ), f"Max diagonal mismatch for {param_set_name}"
    assert np.isclose(
        np.min(dT_dv.diagonal()), expected['sample_values']['min_diagonal'],
        atol=tolerances['derivative']
    ), f"Min diagonal mismatch for {param_set_name}"
    assert np.isclose(
        np.max(np.abs(dT_dv.data)), expected['sample_values']['max_abs'],
        atol=tolerances['derivative']
    ), f"Max absolute value mismatch for {param_set_name}"

    # Test all Jacobian values
    np.testing.assert_allclose(
        dT_dv.toarray(), expected['values'],
        atol=tolerances['derivative'],
        err_msg=f"Bellman operator Jacobian mismatch for {param_set_name}"
    )

    # Test Bellman operator derivatives ∂T/∂θ
    # ---------------------------------------

    dT_dtheta = model.dbellman_operator_dtheta(v)
    expected = baseline['results']['dbellman_operator_dtheta']

    # Test each parameter derivative
    for param_name in model.param_keys:
        param_expected = expected[param_name]
        param_actual = dT_dtheta[param_name]

        # Test shape
        assert param_actual.shape == (model.n_players, model.K), \
            f"Shape mismatch for ∂T/∂{param_name} in {param_set_name}"

        # Test sample values
        sample_expected = param_expected['sample_values']

        assert np.isclose(
            param_actual[0, 0], sample_expected[f'dT_d{param_name}[0,0]'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name}[0,0] mismatch for {param_set_name}"
        assert np.isclose(
            param_actual[0, -1], sample_expected[f'dT_d{param_name}[0,-1]'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name}[0,-1] mismatch for {param_set_name}"
        assert np.isclose(
            param_actual[-1, 0], sample_expected[f'dT_d{param_name}[-1,0]'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name}[-1,0] mismatch for {param_set_name}"
        assert np.isclose(
            param_actual[-1, -1], sample_expected[f'dT_d{param_name}[-1,-1]'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name}[-1,-1] mismatch for {param_set_name}"
        assert np.isclose(
            np.mean(param_actual), sample_expected['mean'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name} mean mismatch for {param_set_name}"
        assert np.isclose(
            np.std(param_actual), sample_expected['std'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name} std mismatch for {param_set_name}"
        assert np.isclose(
            np.max(np.abs(param_actual)), sample_expected['max_abs'],
            atol=tolerances['derivative']
        ), f"∂T/∂{param_name} max_abs mismatch for {param_set_name}"

        # Test all values
        np.testing.assert_allclose(
            param_actual, param_expected['values'],
            atol=tolerances['derivative'],
            err_msg=f"∂T/∂{param_name} values mismatch for {param_set_name}"
        )
